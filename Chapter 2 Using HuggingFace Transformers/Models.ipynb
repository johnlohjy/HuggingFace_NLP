{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModel Class\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.44.0/en/model_doc/auto#auto-classes\n",
    "\n",
    "The AutoModel class and all of its relatives are simple wrappers OVER the wide variety of models available in the library (wrap around the hugging face models)\n",
    "- Wrappers (explanation from ChatGPT): A Python wrapper is a design pattern that involves wrapping an object or function within another object or function to extend or modify its behavior without changing its core structure. Wrappers are commonly used to enhance functionality\n",
    "\n",
    "It can automatically guess the appropriate model architecture for your checkpoint, and instantiate the model with this architecture\n",
    "- Architecture we want to use can be guesssed from the name of the pretrained model supplied to the ```from_pretrained()``` method. Instantiating with the AutoModel directly create a class of the relevant architecture \n",
    "\n",
    "However, if you know the type of model you want to use, you can use the class that defines its architecture directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lets say we know we want to use the BERT model\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/bert\n",
    "\n",
    "Creating a model from the default configuration initializes it with random values\n",
    "\n",
    "The model can be used in this state, but it will output gibberish; it needs to be trained first\n",
    "\n",
    "We could train the model from scratch on the task at hand but this would require a long time and a lot of data\n",
    "\n",
    "To avoid unnecessary and duplicated effort, it’s imperative to be able to share and reuse models that have already been trained\n",
    "\"\"\"\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "# Configuration class that stores the configuration of a BertModel\n",
    "# Used to instantiate a BERT model \n",
    "# Load the configuration object\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "# Initialise a BERT model with the configuration object\n",
    "# Model is randomly initialised\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The configuration contains many attributes used to build the model\n",
    "Examples:\n",
    "- hidden_size attribute defines the size of the hidden_states vector\n",
    "- num_hidden_layers defines the number of layers the Transformer model has\n",
    "\"\"\"\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading a Transformer model that is already trained is simple — we can do this using the from_pretrained() method\n",
    "\n",
    "We could replace BertModel with the equivalent AutoModel class, which is more flexible\n",
    "as it works with any model. More flexible for the future in case you decide to change models, but for a similar task\n",
    "\n",
    "In the code below, we load a pretrained model via the bert-base-cased identifier\n",
    "This is a model checkpoint (saved instance of the model - weights and configurations) that was traind by the authors of BERT themselves\n",
    "Details: https://huggingface.co/google-bert/bert-base-cased\n",
    "- Note: the weights have been downloaded and cached (so future calls to the from_pretrained() method \n",
    "won’t re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. \n",
    "You can customize your cache folder by setting the HF_HOME environment variable.\n",
    "\n",
    "This model is now initialized with all the weights of the checkpoint. \n",
    "It can be used directly for inference on the tasks it was trained on, and \n",
    "it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n",
    "\n",
    "Identifiers\n",
    "- the identifier used to load the model can be the identifier of any model on the Model Hub\n",
    "- Since we are using the BERT model, we can use any identifier compatible with the BERT architecture\n",
    "- List of available BERT checkpoints: https://huggingface.co/models?other=bert\n",
    "\"\"\"\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a model : ```model.save_pretrained(\"directory_on_my_computer\")```\n",
    "\n",
    "This saves 2 files to our disk:\n",
    "1. ```config.json```: Contains attributes necessary to build the model architecture, metadata such as where the checkpoint originated and what huggingface transformers version you were using when you last saved the checkpoint. Necessary to know model's architecture.\n",
    "\n",
    "2. ```pytorch_model.bin```: Known as a state dictionary, contains all the model's weights. Contains model's parameters.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Transformer Model for Inference\n",
    "\n",
    "Transformer models can only process numbers that the tokenizer generates\n",
    "- Tokenizers take care of casting the inputs to the appropriate framework’s tensors\n",
    "\n",
    "We then feed tensors as inputs to the model\n",
    "```output = model(model_inputs)```\n",
    "\n",
    "While the model accepts a lot of different arguments, only the input IDs are necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
